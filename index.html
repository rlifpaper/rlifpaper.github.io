<html>
    <head>
        <style>
            body {
                padding: 5em;
                max-width: 1200px;
                margin: auto;
                text-align: left;
                font-family: 'Verdana',Roboto,arial,sans-serif;
            }
            p {
                font-weight: 100;
                font-size: 1.1em;
                text-align: left;
            }
            ul {
              font-weight: 100;
              font-size: 1.1em;
              text-align: left;
            }
            li {
              margin-bottom:  2px;
            }
            h1 {
                display: block;
                font-size: 2em;
                margin-block-start: 0.67em;
                margin-block-end: 0.67em;
                margin-inline-start: 0px;
                margin-inline-end: 0px;
                font-weight: 400;
            }
            h2 {
                font-weight: 400;
            }
            .special {
                align: center;
                max-width: 50%;
                width: 50%;
                height: auto;
                margin-left: auto;
                margin-right: auto;
            }

            .special-large {
                align: center;
                max-width: 80%;
                width: 80%;
                height: auto;
                margin-left: auto;
                margin-right: auto;
            }

            figure {
                width:50%;
            }
            figure figcaption {
              border: 1px dotted blue;
              text-align: center;
            }
            tr {
                display: table-row;
                vertical-align: inherit;
                border-color: inherit;
            }
            img {
                width: 100%;
                height: 100%;
                object-fit: cover;
            }
            /* .author-block {
                display: inline-block;
            } */

          .publication-authors {
              font-family: 'Google Sans', sans-serif;
          }
          .publication-links {
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 3em;
            margin-top: 1em
            /* width: 50%; */
            /* gap: 10px; */
          }
          .link-block {
            height: 40px;
            width: 50px;
            margin: 20px;
            /* flex-basis: 100%; */
            /* width: 0; */


          }
          .authors {
            display: flex;
            align-items: center;
            align-self: center;
            align-content: center;
            justify-content: center;
            /* width: 50%; */
            /* gap: 10px; */
            margin: 1.5em;
            margin-bottom: 0.5em;
            font-size: 20;
            margin-left: 0;
            margin-right: 0;
          }
          /* .author-block {
            height: 10px;
            width: 150px;
            margin: 0px;
            align: center;
          } */
          .authors div {
            flex: 0 0 auto;
          }
          .first{
            width: 150px;
          }
          .second {
            width: 148px;
          }
          .third {
            width: 144px;
          }
          .fourth {
            width: 162px;
          }
          .fifth {
            width: 170px;
          }
          .sixth {
            width: 170px;
          }
          .author-block1 {
            height: 10px;
            width: 150px;
            margin: 0px;
          }
          img { width: 100%; height: auto; }
          a{
            color: #3273dc;
    cursor: pointer;
    text-decoration: none;
    text-decoration-line: none;
    text-decoration-thickness: initial;
    text-decoration-style: initial;

          }
        </style>
        <title>RLIF: Interactive Imitation Learning as Reinforcement Learning
        </title>
    </head>
    <body>
        <center>
            <h1>RLIF: Interactive Imitation Learning as Reinforcement Learning
            </h1>
        </center>

        <!-- <div class="authors">
          <div class="first">
              <a style="text-decoration:none" href="https://people.eecs.berkeley.edu/~jianlanluo/">Jianlan Luo</a></div>
          <div class="second">
              <a style="text-decoration:none" href="https://pd-perry.github.io">Perry Dong</a></div>
          <div class="third">
              <a style="text-decoration:none" href="https://jeffreywu13579.github.io">Jeffrey Wu</a>
          </div>
          <div class="fourth">
              <a style="text-decoration:none" href="https://aviralkumar2907.github.io/">Aviral Kumar</a>
          </div>
          <div class="fifth"z>
              <a style="text-decoration:none" href="https://young-geng.xyz/">Xinyang Geng</a>
          </div>
          <div class="sixth">
              <a style="text-decoration:none" href="https://cs.berkeley.edu/~svlevine">Sergey Levine</a>
          </div>
      </div> -->


          <div class="publication-links">

              <!-- Code Link. -->
              <div class="link-block">
                  <a href="">
                      <img src="github.png"></img>
                      <center><span>Code</span></center>
                      <center><span width='150px'>(coming soon)</span></center>
                  </a>
              </div>


              <!-- BibTex -->
              <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2310.11731.pdf">
                      <img src="paper.png"></img>
                      <center><span>Paper</span></center>
                  </a>
              </span> -->

          </div>

<br>
        <center>
            <h2>Abstract
            </h2>
        </center>

        <center>
            <p>Although reinforcement learning methods offer a powerful framework for
              automatic skill acquisition, for practical learning-based control problems in domains
              such as robotics, imitation learning often provides a more convenient and accessible
              alternative. In particular, an interactive imitation learning method such as DAgger,
              which queries an expert to intervene online to collect correction data for addressing
              the distributional shift challenges that afflict na√Øve behavioral cloning, can enjoy
            good performance both in theory and practice without requiring manually specified
            reward functions and other components of full reinforcement learning methods. In
            this paper, we explore how off-policy reinforcement learning can enable improved
            performance under assumptions that are similar but potentially even more practical
            than those of interactive imitation learning. Our proposed method uses
            reinforcement learning with rewards derived automatically from user intervention signals.
            This relaxes the requirement that user interventions in interactive imitation learning
            should be optimal, and enables the algorithm to learn behaviors that improve over
            the human expert, without requiring significantly stronger assumptions that those
            commonly made in interactive imitation learning algorithms. We analyze the regret
            of our method and its imitation learning counterpart under different intervention
            modes. We show that, under even weaker assumptions on the optimality of
            intervention actions, our method attains better performance bounds in theory. We
            then evaluate our method on challenging high-dimensional continuous control
            benchmarks. The results show that it outperforms DAgger-like approaches across
            the different tasks, especially when the intervening experts are suboptimal.
            </p>
            <br>
        </center>

        <!-- <center>
            <h2>Summary
            </h2>
        </center>

        <ul>
          <li>Train a conditional VQ-VAE to learn a latent representation of the actions conditioned on the state</li>
          <li>Perform offline RL with the VQ-VAE discrete codes as actions</li>
          <li>During inference, select the best discrete action using the policy and transform into continuous with the trained decoder</li>
        </ul>

        <div wdith="50%">
        <img src="saq_static.jpeg">
      </div> -->

      <br>

      <center>
          <h2>Method
          </h2>
      </center>


        <div wdith="50%">
        <img src="rlif.gif">
      </div>

      <br>

      <center>
          <h2>Robot Experiments
          </h2>
      </center>


        <div wdith="40%">
        <img src="saq_results.jpeg">
      </div>



    </body>
</html>
